{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb6Ijyh-a9uk"
   },
   "source": [
    "## Data Simulation\n",
    "* Here we simulate the production line data in three different scales: 500, 1000, and 2000 to investigate the performance of the score-based causal discovery algorithms on different scales of data.\n",
    "* The boostrapping is conducted with 10 iterations on the sample size of 500 due to the resource limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simulated dataset...\n",
      "Retrieving ground truth DAG...\n",
      "Filtered dataset with 500 samples saved as 'station_data_500.csv'.\n",
      "Filtered dataset with 1000 samples saved as 'station_data_1000.csv'.\n",
      "Filtered dataset with 2000 samples saved as 'station_data_2000.csv'.\n",
      "Ground truth adjacency matrix for Stations 1-3 saved as 'station_ground_truth.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from causalAssembly.models_dag import ProductionLineGraph\n",
    "\n",
    "# Define sample sizes\n",
    "sample_sizes = [500, 1000, 2000]\n",
    "\n",
    "# Step 1: Generate the simulated dataset\n",
    "print(\"Generating simulated dataset...\")\n",
    "assembly_line_data = ProductionLineGraph.get_data()\n",
    "\n",
    "# Step 2: Retrieve the ground truth DAG\n",
    "print(\"Retrieving ground truth DAG...\")\n",
    "assembly_line_ground_truth = ProductionLineGraph.get_ground_truth()\n",
    "\n",
    "# Step 3: Filter dataset columns for Stations 1, 2, and 3\n",
    "station_prefixes = [\"Station1_\", \"Station2_\", \"Station3_\"]\n",
    "filtered_columns = [\n",
    "    col for col in assembly_line_data.columns if any(col.startswith(prefix) for prefix in station_prefixes)\n",
    "]\n",
    "\n",
    "# Step 4: Generate datasets for three sample sizes\n",
    "for size in sample_sizes:\n",
    "    # Create a filtered dataset for the current sample size\n",
    "    sampled_data = assembly_line_data[filtered_columns].sample(n=size, random_state=42)\n",
    "    \n",
    "    # Save the filtered dataset to a CSV file\n",
    "    file_name = f\"station_data_{size}.csv\"\n",
    "    sampled_data.to_csv(file_name, index=False)\n",
    "    print(f\"Filtered dataset with {size} samples saved as '{file_name}'.\")\n",
    "\n",
    "# Step 5: Filter the ground truth graph for Stations 1, 2, and 3\n",
    "station_nodes = [\n",
    "    node for node in assembly_line_ground_truth.ground_truth.index\n",
    "    if any(node.startswith(prefix) for prefix in station_prefixes)\n",
    "]\n",
    "\n",
    "# Create a filtered adjacency matrix\n",
    "filtered_adjacency_matrix = assembly_line_ground_truth.ground_truth.loc[station_nodes, station_nodes]\n",
    "\n",
    "# Save the filtered adjacency matrix to a CSV file\n",
    "filtered_adjacency_matrix.to_csv(\"station_ground_truth.csv\", index=True)\n",
    "print(\"Ground truth adjacency matrix for Stations 1-3 saved as 'station_ground_truth.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
